{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nombre1:\n",
    "Nombre2:    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistemas de Gestión de Datos y de la Información \n",
    "\n",
    "## Práctica 2\n",
    "\n",
    "- Se valorará la claridad del código y evitar redundancias o código poco eficiente; en particular se valorará lograr el resultado de las consultas mediante MongoDB minimizando el uso de Python \n",
    "- Además de las funciones que se piden se pueden añadir otras auxiliares si se necesitan, y también otros imports\n",
    "- El código debe funcionar correctamente no solo con las pruebas que vienen de ejemplo sino con cualquier otra prueba\n",
    "\n",
    "\n",
    "### Primera parte: MongoDB\n",
    "\n",
    "Se requiere tener acceso a un servidor, ya sea arrancado en local o en la nube. Comprobar con el siguiento código si se puede acceder a él:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymongo está en el sistema!\n",
      "pprint está en el sistema!\n",
      "Conectado a MongoDB, versión 5.0.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cambiar si hace falta, ahora está para servidor local\n",
    "#url_servidor = 'mongodb://127.0.0.1:27017/'\n",
    "url_servidor = 'mongodb://127.0.0.1:27017/'\n",
    "\n",
    "# si es en atlas será algo como\n",
    "#url_servidor= \"mongodb+srv://aniceto:castañas@cluster0.nubot.mongodb.net/test?retryWrites=true&w=majority\"\n",
    "\n",
    "import sys\n",
    "# comprobar si pymongo está instalado, y hacerlo en otro caso\n",
    "try:\n",
    "    import pymongo\n",
    "    print(\"pymongo está en el sistema!\")\n",
    "except ImportError as e:\n",
    "    !{sys.executable} -m pip install --upgrade --user pymongo\n",
    "    import pymongo\n",
    "\n",
    "try:\n",
    "    import pprint\n",
    "    print(\"pprint está en el sistema!\")\n",
    "except ImportError as e:\n",
    "    !{sys.executable} -m pip install --upgrade --user pprint\n",
    "    import pprint\n",
    "\n",
    "from pprint import pprint # para mostrar los json bonitos\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Atlas: \n",
    "#client = MongoClient(\"mongodb+srv://aniceto:castañas@cluster0.nubot.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "client = MongoClient(url_servidor)\n",
    "\n",
    "# código para ver si se ha conectado bien\n",
    "try:\n",
    "    s = client.server_info() # si hay error tendremos una excepción\n",
    "    print(\"Conectado a MongoDB, versión\",s[\"version\"])\n",
    "except:\n",
    "    print (\"Error de conexión ¿está arrancado el servidor?\")\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es importar los datos. Una posibilidad es bajarse el fichero de JSON e importarlo con mongoimport, pero aquí vamos a ver otra forma de hacerlo que presenta la ventaja de que nos permite trabajar con ficheros realmente grandes (no es el caso) porque hace carga \"perezosa\" , documento a documento, sin llegar a tener todos los documentos en memoria en ningún momento.\n",
    "\n",
    "**Ojo**: este código borrará la colección tweet de la base de datos practica2, antes de cargar el contenido, para evitar que el código sea acumulativo (un matemático diría que para que el código sea idempotente)\n",
    "\n",
    "La salida esperada\n",
    "\n",
    "    Colección tweet: 3022 documentos cargados con éxito y 0 errores\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colección tweet: 3022 documentos cargados con éxito y 0 errores\n"
     ]
    }
   ],
   "source": [
    "import json # para transformar la línea leida (string) a json\n",
    "import urllib.request # para leer de la URL línea a línea\n",
    "\n",
    "def carga_desde_fichero(db,fichero,coleccion):\n",
    "    db[coleccion].drop() # la borramos\n",
    "    exito,error = 0,0\n",
    "    # cargamos los datos desde el fichero\n",
    "    try:\n",
    "        with urllib.request.urlopen(fichero) as f:\n",
    "        \n",
    "            for line in f:\n",
    "                line2 = line.decode(\"UTF-8\").replace(\"$\",\"\")\n",
    "                res = db[coleccion].insert_one(json.loads(line2))\n",
    "                if res.acknowledged:\n",
    "                    exito+=1\n",
    "                else:\n",
    "                    error+=1\n",
    "        print(f\"Colección {coleccion}: {exito} documentos cargados con éxito y {error} errores\")\n",
    "    except urllib.error.URLError as e:\n",
    "        print(e.reason)\n",
    "   \n",
    "            \n",
    "db = client.practica2\n",
    "fichero_tweets = \"https://raw.githubusercontent.com/RafaelCaballero/tdm/master/datos/ctweet.json\"\n",
    "carga_desde_fichero(db,fichero_tweets,\"tweet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1** Escribir una función `num_replicas` que devuelva cuántos tweets son réplicas (clave `in_reply_to_screen_name`) a un usuario.\n",
    "\n",
    "- *Nombre*: `num_replicas`\n",
    "- *Parámetros de entrada*: \n",
    "    - `db`: el acceso a una base de datos que se asume incluye una colección `tweet`de tweets\n",
    "    - `usuario`: screen_name de un usuario\n",
    "   \n",
    "- *Devuelve*: número de tweets que son réplicas al usuario (un entero mayor o igual que 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 réplicas a JoeBiden\n",
      "204 réplicas a realDonaldTrump\n"
     ]
    }
   ],
   "source": [
    "# solución\n",
    "def num_replicas(db,usuario):\n",
    "\n",
    "### para probar el código\n",
    "print(num_replicas(db,\"JoeBiden\"),\"réplicas a JoeBiden\")\n",
    "print(num_replicas(db,\"realDonaldTrump\"),\"réplicas a realDonaldTrump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2** Escribir una función `num_menciones` para conocer el número de tweets que nombran (examinar `user_mentions`) a un usuario dado su screen_name \n",
    "- *Nombre*: `num_menciones`\n",
    "- *Parámetros de entrada*: \n",
    "    - `db`: el acceso a una base de datos que se asume incluye una colección `tweet`de tweets\n",
    "    - `usuario`: screen_name de un usuario\n",
    "   \n",
    "- *Devuelve*: número de tweets que mencionan al usuario (un entero mayor o igual a 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453 menciones a JoeBiden\n",
      "1479 menciones a realDonaldTrump\n"
     ]
    }
   ],
   "source": [
    "# solución\n",
    "def num_menciones(db,usuario):\n",
    "\n",
    "    \n",
    "### para probar el código\n",
    "print(num_menciones(db,\"JoeBiden\"),\"menciones a JoeBiden\")\n",
    "print(num_menciones(db,\"realDonaldTrump\"),\"menciones a realDonaldTrump\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejericicio 3** Escribir una función `prop_clave` para conocer la proporción de documentos que contienen una cierta clave a primer nivel en la colección tweets\n",
    "- *Nombre*: `prop_clave`\n",
    "- *Parámetros de entrada*: \n",
    "    - `db`: el acceso a una base de datos que se asume incluye una colección `tweet`de tweets\n",
    "    - `clave`: clave a comprobar\n",
    "   \n",
    "- *Devuelve*: proporción sobre 1 (número entre 0 y 1) de tweets que incluyen la clave\n",
    "- Obs: se puede usar python para hacer la operación aritmética que da la proporción, no tiene sentido usar MongoDB para eso\n",
    "\n",
    "En el ejemplo se prueba la prop. de tweets que tienen coordenadas (esto es incluyen la clave 'coordinates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proporción de tweets con coordenadas 0.6035737921906023\n"
     ]
    }
   ],
   "source": [
    "# solución\n",
    "def prop_clave(db,clave):\n",
    "\n",
    "    \n",
    "print(\"Proporción de tweets con coordenadas\",prop_clave(db,\"coordinates\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejericicio 4** Escribir una función `muestra_NY(db)` que internamente utilice una consulta Mongo para obtener los tweets que se han generado entre la longitud -74.5 y -73.5 (primera coordenada), y la latitud 40.5 y 41 (segunda coordenada), con los valores extremos no incluídos, y muestre, usando folium, un mapa centrado en las coordenadas 40.75 (lat) y -74(long), con un zoom_start de 9 (lo que corresponde al área de NY), tal que al hacer click en cada tweet muestre su texto\n",
    "\n",
    "- *Nombre*: `muestra_NY(db)` \n",
    "- *Parámetros de entrada*: \n",
    "\n",
    "    - db el acceso a la base de datos\n",
    "    \n",
    "- *Devuelve*: Un map de folium con la representación de los tweets en su coordenada. Al hacer click sobre un tweet se mostrará su text\n",
    "\n",
    "Debe mostrar algo parecido a esto (en el ejemplo tras hacer click en un tweet)\n",
    "![](http://gpd.sip.ucm.es/rafa/docencia/nosql/images/ny.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# solución\n",
    "def muestra_NY(db):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para probar \n",
    "muestra_NY(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 5** La librería para el tratamiento de lenguaje natural `flair` incorpora, entre otras muchas utilidades una sencilla forma de hacer análisis de sentimiento.\n",
    "\n",
    "Primero, si no hemos intalado la librería debemos hacerlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: flair in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (0.9)\n",
      "Requirement already satisfied, skipping upgrade: regex in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from flair) (2020.10.15)\n",
      "Requirement already satisfied, skipping upgrade: sqlitedict>=1.6.0 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from flair) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: transformers>=4.0.0 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (4.10.2)\n",
      "Requirement already satisfied, skipping upgrade: deprecated>=1.2.4 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (1.2.13)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece==0.1.95 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (0.1.95)\n",
      "Requirement already satisfied, skipping upgrade: segtok>=1.5.7 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (1.5.10)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.26.0 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from flair) (4.50.2)\n",
      "Requirement already satisfied, skipping upgrade: langdetect in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (1.0.9)\n",
      "Requirement already satisfied, skipping upgrade: gensim<=3.8.3,>=3.4.0 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from flair) (0.8.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from flair) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: wikipedia-api in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (0.5.4)\n",
      "Requirement already satisfied, skipping upgrade: bpemb>=0.3.2 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: ftfy in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (6.0.3)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from flair) (3.3.2)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools~=8.8.0 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (8.8.0)\n",
      "Requirement already satisfied, skipping upgrade: konoha<5.0.0,>=4.0.0 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (4.6.5)\n",
      "Requirement already satisfied, skipping upgrade: lxml in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from flair) (4.6.1)\n",
      "Requirement already satisfied, skipping upgrade: gdown==3.12.2 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (3.12.2)\n",
      "Requirement already satisfied, skipping upgrade: janome in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (0.0.17)\n",
      "Requirement already satisfied, skipping upgrade: torch!=1.8,>=1.5.0 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: mpld3==0.3 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (0.3)\n",
      "Requirement already satisfied, skipping upgrade: conllu>=4.0 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: hyperopt>=0.1.1 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from flair) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from transformers>=4.0.0->flair) (0.10.3)\n",
      "Requirement already satisfied, skipping upgrade: filelock in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from transformers>=4.0.0->flair) (0.0.45)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from langdetect->flair) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.14 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from gensim<=3.8.3,>=3.4.0->flair) (0.29.14)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from gensim<=3.8.3,>=3.4.0->flair) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2020.06.20 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (8.0.1)\n",
      "Requirement already satisfied, skipping upgrade: overrides<4.0.0,>=3.0.0 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata<4.0.0,>=3.7.0 in c:\\users\\rafa\\appdata\\roaming\\python\\python38\\site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from huggingface-hub->flair) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.2 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
      "Requirement already satisfied, skipping upgrade: future in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from requests->transformers>=4.0.0->flair) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from requests->transformers>=4.0.0->flair) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from requests->transformers>=4.0.0->flair) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in c:\\users\\rafa\\anaconda3\\lib\\site-packages (from networkx>=2.2->hyperopt>=0.1.1->flair) (4.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí un pequeño ejemplo; la función `sentimiento` (que podemos usar en el ejercicio) para cada texto nos devuelve el sentimiento general y el grado de confianza en su estimación. La función recibie el clasificador (que no debemos cambiar) y la frase, y devuelve el valor y la confianza en forma de diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-19 18:20:28,351 loading file C:\\Users\\Rafa\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n",
      "Sentimiento  NEGATIVE  confianza  0.9916785955429077\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "\n",
    "def sentimiento(classifier, s):\n",
    "    # make a sentence\n",
    "    sentence = Sentence(s)\n",
    "    classifier.predict(sentence)\n",
    "    return sentence.labels[0].to_dict()\n",
    "frase = '''Since 2016, the SEC/FBI has used 10 tax payer funded lawyers to shake me down over a 35,000 civil trade \n",
    "       dispute, all because I support @realDonaldTrump. \n",
    "       While ignoring @JoeBiden family schemes to bring billions from China, UKR &amp; Russia. \n",
    "       If I was a @TheDemocrats I’d blame RACISM! https://t.co/5rwMrGihcl'''\n",
    "\n",
    "s = sentimiento(classifier,frase)\n",
    "print(\"Sentimiento \",s[\"value\"],\" confianza \",s[\"confidence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribir una función `añade_sentimiento(db,classifier,screen_name)` que modifique los tweets del usuario indicado que tengan una clave text añadiendo el sentiemiento que indica flair en una nueva clave `flair`. Además, función devolverá el sentimiento medio del usuario sumando todas las \"confidences\" pero con valor negativo para las que tengan \"value\" \"NEGATIVE\", con valor positivo para las que lo tengan \"POSITIVE\", y con valor 0 para los que devuelvan \"NEUTRAL\", y dividiendo finalmente esta suma por el total de valores sumados.\n",
    "\n",
    "\n",
    "- *Nombre*: `añade_sentimiento(db,classifier,screen_name)` \n",
    "- *Parámetros de entrada*: \n",
    "\n",
    "    - db el acceso a la base de datos\n",
    "    - classifier: el clasificador flair\n",
    "    - screen_name: el screen_name de un usuario\n",
    "    \n",
    "- *Devuelve*: Modifica la colección tweet añadiendo a cada documento del usuario que contenga la clave 'text' el sentimiento devuelto por flair. Además devuelve la media de las \"confidences\" considerando las negativas con un -, las positivas como valores positivas y las neutrales como 0\n",
    "\n",
    "Nota: la media se puede calcular sobre la marcha en Python, ya que va obteniendo cada \"confidence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solución\n",
    "def añade_sentimiento(db,classifier,screen_name):\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2596050142378047\n",
      "0.9064595213642826\n"
     ]
    }
   ],
   "source": [
    "# para probar\n",
    "db.tweet.update_many({},{\"$unset\":{\"flair\":\"\"}})\n",
    "print(añade_sentimiento(db,classifier,\"realDonaldTrump\"))\n",
    "print(añade_sentimiento(db,classifier,\"jon_kinsley\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 6** Escribir una función `num_tweets(db,n)`para mostrar por pantalla el número de tweets de cada usuario de mayor a menor para todo usuario que haya escrito al menos n tweets\n",
    "\n",
    "- *Nombre*: `num_tweets(db,n)` \n",
    "- *Parámetros de entrada*: \n",
    "\n",
    "    - db el acceso a la base de datos\n",
    "    - n el número mínimo de tweets para que un usuario aparezca en e resultado\n",
    "    \n",
    "- *Devuelve*: nada, solo muestra el resultado por pantalla\n",
    "\n",
    "Nota: se valorará hacerlo usando agregaciones (ver notebook pymongo-aggregaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'LumberjackHill', 'num_tweets': 336}\n",
      "{'_id': 'DaysLeft4Trump', 'num_tweets': 173}\n",
      "{'_id': 'realDonaldTrump', 'num_tweets': 138}\n",
      "{'_id': 'shiffy64', 'num_tweets': 90}\n",
      "{'_id': 'pidybi', 'num_tweets': 69}\n",
      "{'_id': 'jon_kinsley', 'num_tweets': 55}\n"
     ]
    }
   ],
   "source": [
    "# ejercicio 6\n",
    "def num_tweets(db,n):\n",
    "\n",
    "num_tweets(db,50)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejericicio 7**   **Difícil** \n",
    "Definir una función `cercanos(db,long,lat,maxKm)` que permite obtener una lista de los screen_name de los usuarios que se han enviado algún tweet a una distancia máxima `maxKm`de ese punto\n",
    "\n",
    "- *Nombre*: `cercanos(db,long,lat,maxKm)` \n",
    "- *Parámetros de entrada*: \n",
    "\n",
    "    - db el acceso a la base de datos\n",
    "    - long,lat: coordenadas de un punto (en el mismo formato que se usa en los tweets)\n",
    "    - Distancia máxima en la que buscar tweets\n",
    "    \n",
    "    \n",
    "- *Devuelve*: Una lista Python con los screen_names de los usuarios que emitieron tweets a una distancia máxima maxKM. Si algún usuario ha emitido más de un tweet con coordenadas desde menos de esa distancia, aparecerá repetido\n",
    "\n",
    "Nota: se debe hacer utilizando funciones de Mongo y aprovechando el índice de tipo GEOSPHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HW_TRK']\n",
      "['Brian_Atwood', 'trafficgifs', 'TimEBrutus', 'LionelMedia', 'ngalai', 'TheatreChat', 'sailorboyj', 'realDonaldTrump', 'realDonaldTrump', 'realDonaldTrump', 'realDonaldTrump', 'realDonaldTrump', 'realDonaldTrump', 'realDonaldTrump']\n"
     ]
    }
   ],
   "source": [
    "# No borrar esto; si se pueden añadir funciones auxiliares u otros import que hagan falta a continuación\n",
    "db.tweet.create_index([( \"coordinates\", pymongo.GEOSPHERE)])\n",
    "\n",
    "def cercanos(db,long,lat,maxKm):\n",
    "        \n",
    "\n",
    "print(cercanos(db,-117,0,5))\n",
    "print(cercanos(db,-74,40.75,2.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2 - Neo4j\n",
    "\n",
    "Estos ejercicios no los ejecutaremos en el Notebook sino en Neo4j, aquí copiaremos solo la solución\n",
    "\n",
    "Para comenzar, copiar y pegar las instrucciones para crear un [grafo de ejemplo](https://raw.githubusercontent.com/RafaelCaballero/tdm/master/datos/usa2020neo4j.txt) con usuarios que han retuiteado a otros. El grafo tendrá un aspecto similar a este:\n",
    "\n",
    "![](http://gpd.sip.ucm.es/rafa/docencia/nosql/images/usa2020neo4j.png)\n",
    "\n",
    "Todos los nodos son de tipo Usuario, solo que algunos son además de tipo BP (Biden positivo) o TN (Trump negativo) o ambos. Las relaciones pueden ser de cualquiera de los 4 tipos que se indican en la gráfica, y van de la persona que retuitea al autor del mensaje. Es decir, si A reuitea a B se tendra una relación de la forma (A)-->(B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1** Obtener todos los usuarios que han retuiteado  al usuario con atributo `id` \"MichaelCohen212\" (salen 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiar aquí la solución\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2** Encontrar un camino de longitud 4 (4 relaciones, 5 nodos) entre los usuarios con id \"CaslerNoel\" y \"mmpadellan\". El orden de las flechas no importa, pero todas las relaciones deben ser de tipo TNegBNeu  (Trump negativo, Biden neutro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiar aquí la solución\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3**  id del usuario con más followers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiar aquí la solución\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 4** Encontrar un ciclo de longitud 4 (4 relaciones, 4 nodos ya que el primero y el último son el mismo) entre el usuario id:\"DHStokyo\" y el mismo, de forma que los 4 nodos tengan diferentes `id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiar aquí la solución\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
